{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "\n",
    "# MNIST 데이터 관련 import\n",
    "from keras.datasets import mnist                  # MNIST 데이터 Loader\n",
    "from keras.utils.np_utils import to_categorical   # One-hot 포맷 변환\n",
    "import numpy as np                                # float type casting\n",
    "\n",
    "# Feature scaling 관련 import\n",
    "from sklearn.preprocessing import minmax_scale    # [0-1] Scaling\n",
    "\n",
    "# Model 구축 관련 import\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# 데이터 Load 및 전처리 과정\n",
    "\n",
    "# Train, Test 데이터 Load\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Train 데이터 포맷 변환\n",
    "# 60000(Train Sample 수) * 28(가로) * 28(세로) 포맷을\n",
    "# 60000(Train Sample 수) * 784(= 28 * 28) 포맷으로 수정\n",
    "num_of_train_samples = X_train.shape[0]                     # Train Sample 수\n",
    "width = X_train.shape[1]                                    # 가로 길이\n",
    "height = X_train.shape[2]                                   # 세로 길이\n",
    "X_train = X_train.reshape(num_of_train_samples, width * height)\n",
    "\n",
    "# Test 데이터 포맷 변환\n",
    "# width, height는 Train 데이터와 같으므로 재사용\n",
    "# 10000(Test Sample 수) * 28(가로) * 28(세로) 포맷을\n",
    "# 10000(Test Sample 수) * 784(= 28 * 28) 포맷으로 수정\n",
    "num_of_test_samples = X_test.shape[0]  # Sample 수\n",
    "X_test = X_test.reshape(num_of_test_samples, width * height)\n",
    "\n",
    "# Feature Scaling\n",
    "# X_train의 각 원소는 0-255 사이의 값을 가지고 있다\n",
    "# Overfitting 방지 및 Cost 함수의 빠른 수렴을 위해서 \n",
    "# Feature Scaling 작업을 한다.\n",
    "# 예제에서는 0-255 범위를 0-1 범위로 Scaling\n",
    "# 참고: https://en.wikipedia.org/wiki/Feature_scaling\n",
    "\n",
    "# 나누기 연산이 들어가므로 uint8을 float64로 변환한다\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "# 간단한 방법은 MNIST가 0-255 사이 값만을 가진다는 것을 알기 때문에\n",
    "# 단순히 255를 나눠도 Feature Scaling이 가능하다.\n",
    "# X_train = X_train / 255.0\n",
    "# X_test = X_test / 255.0\n",
    "\n",
    "# 아래 방법은 다소 복잡하지만 다른 데이터에서도 동일하게 적용할 수 있음\n",
    "# Sample by featre matrix 형태이므로 axis=0로 설정\n",
    "# axis=1은 축을 바꿔서 scaling, 자세한 내용은 scikit 문서 참조\n",
    "X_train = minmax_scale(X_train, feature_range=(0, 1), axis=0)\n",
    "X_test = minmax_scale(X_test, feature_range=(0, 1), axis=0)\n",
    "\n",
    "# Lable의 categorical 값을 One-hot 형태로 변환\n",
    "# 예를 들어 [1, 3, 2, 0] 를\n",
    "# [[ 0.,  1.,  0.,  0.],\n",
    "#  [ 0.,  0.,  0.,  1.],\n",
    "#  [ 0.,  0.,  1.,  0.],\n",
    "#  [ 1.,  0.,  0.,  0.]]\n",
    "# 로 변환하는 것을 One-hot 형태라고 함\n",
    "# MNIST Label인 0 ~ 9사이의 10가지 값을 변환한다.\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.3973 - acc: 0.8776    \n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.1708 - acc: 0.9507    \n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 9s - loss: 0.1370 - acc: 0.9611     \n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.1139 - acc: 0.9674    \n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 9s - loss: 0.0996 - acc: 0.9705     \n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.0923 - acc: 0.9736    \n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 12s - loss: 0.0801 - acc: 0.9769    \n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 12s - loss: 0.0770 - acc: 0.9775    \n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.0737 - acc: 0.9780    \n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.0656 - acc: 0.9809    \n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.0640 - acc: 0.9807    \n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.0584 - acc: 0.9827    \n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.0583 - acc: 0.9829    \n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 11s - loss: 0.0536 - acc: 0.9838    \n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 10s - loss: 0.0539 - acc: 0.9845    \n",
      "모델 평가\n",
      "10000/10000 [==============================] - 0s     \n",
      "Accuracy: 0.981900011301\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Multilayer Perceptron (MLP) 생성\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Dense(256)의 의미는 256개의 hidden unit을 가지는 fully connected layer\n",
    "# keras에서는 첫 번째 Layer, 즉 input layer의 input dimension만 지정하면\n",
    "# 뒤의 연결되는 Layer의 dimension은 간단하게 작성 가능하다.\n",
    "\n",
    "# width * height = 784인 dimension\n",
    "# glorot_uniform == Xavier Initialization, keras에서는 내부적으로 이미 제공\n",
    "# 그 외 he_uniform 등도 이미 구현되어있다.\n",
    "\n",
    "# 첫 번째 Layer (Input layer)\n",
    "model.add(Dense(256, input_dim=width * height, init='glorot_uniform'))\n",
    "model.add(Activation('relu'))  # Activation 함수로 relu 사용\n",
    "model.add(Dropout(0.3))        # 30% 정도를 Drop\n",
    "\n",
    "# 두 번째 Layer (Hidden layer 1)\n",
    "# 두 번째 Layer부터는 output_dim만 설정하면 된다\n",
    "# input_dim은 이전 레이어의 output_dim과 같다고 가정함\n",
    "model.add(Dense(256, init='glorot_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 세 번째 Layer (Hidden layer 2)\n",
    "model.add(Dense(256, init='glorot_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 네 번째 Layer (Hidden layer 3)\n",
    "model.add(Dense(256, init='glorot_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# 다섯 번째 Layer (Output layer)\n",
    "# Output layer는 softmax activation function\n",
    "number_of_class = 10  # MNIST 예제는 10가지의 Category를 가지고 있다.\n",
    "model.add(Dense(number_of_class, activation='softmax'))  \n",
    "\n",
    "# Cost function 및 Optimizer 설정\n",
    "# Multiclass 분류이므로 Cross-entropy 사용\n",
    "# Adam optimizer 사용\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model training\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "model.fit(X_train, y_train, nb_epoch=training_epochs, batch_size=batch_size)\n",
    "\n",
    "# Model evaluation using test set\n",
    "print('모델 평가')\n",
    "evaluation = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print('Accuracy: ' + str(evaluation[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
